{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9da81c-b7c9-45c9-bfb8-bb0d12cec459",
   "metadata": {},
   "source": [
    "```{margin} Adaptation!\n",
    "This lecture was adapted with permission from Prof. AJ Medford (GTech)'s lectures for ChBE4745: https://github.com/medford-group/data_analytics_ChE\n",
    "\n",
    "The dataset came from Dow Chemicals and released publicly as part of Prof. Medford's class.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4435f-bef1-499f-ae1b-fc8af53ba299",
   "metadata": {},
   "source": [
    "`````{note}\n",
    "This lecture is going to:\n",
    "* introduce the functional form of neural networks\n",
    "* code a neural network from scratch using numpy/scipy\n",
    "* show why gradients are so important for fitting models with many parameters\n",
    "* improve the efficiency of regression using automatic differentiation\n",
    "* discuss some of the logistical considerations of using and training neural networks\n",
    "    * mini-batch\n",
    "    * gradient descent\n",
    "    * packages, accelerators, etc\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd8fa4-4ca7-40a5-903c-a2e165fd34fa",
   "metadata": {},
   "source": [
    "## Unsupervised learning: dimensionality reduction\n",
    "\n",
    "Let's remind ourselves what supervised and unsupervised means in the context of machine learning:\n",
    "* **supervised:** We are building models that have some input data/features and some known output target/label (a number if regression, a categorical variable if classification)\n",
    "* **unsupervised:** We want models that only use the input data/features.\n",
    "\n",
    "Since we're often trying to predict something in engineering we've focused on supervised techniques so far. However, unsupervised learning is very helpful. The two most common use cases are:\n",
    "* **dimensionality reduction:** We want to reduce the number of input features\n",
    "* **clustering:** We want to cluster some data to find similar points in a dataset or understand the data distribution\n",
    "\n",
    "We're going to talk about dimensionality reduction today!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53865fa-6dd5-4a94-92ac-5d9a08338463",
   "metadata": {},
   "source": [
    "## Dataset: Dow process impurity\n",
    "\n",
    "We're going to use the Dow process dataset that you saw on your homework already. \n",
    "\n",
    "\n",
    "![DOW process](resources/dow_process.png)\n",
    "\n",
    "The dataset contains a number of operating conditions for each of the units in the process, as well as the concentration of impurities in the output stream. We'll use the same filtering that you used on your homework to remove some problematic data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eea60-8969-4297-81ca-7faa3eefde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"datasets/impurity_dataset-training.xlsx\")\n",
    "\n",
    "\n",
    "def is_real_and_finite(x):\n",
    "    if not np.isreal(x):\n",
    "        return False\n",
    "    elif not np.isfinite(x):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "all_data = df[df.columns[1:]].values  # drop the first column (date)\n",
    "numeric_map = df[df.columns[1:]].applymap(is_real_and_finite)\n",
    "real_rows = (\n",
    "    numeric_map.all(axis=1).copy().values\n",
    ")  # True if all values in a row are real numbers\n",
    "X = np.array(\n",
    "    all_data[real_rows, :-5], dtype=\"float\"\n",
    ")  # drop the last 5 cols that are not inputs\n",
    "y = np.array(all_data[real_rows, -3], dtype=\"float\")\n",
    "\n",
    "x_names = [str(x) for x in df.columns[1:41]]\n",
    "y_name = str(df.columns[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a95b2b-a8d2-4efb-b273-cc9b38020893",
   "metadata": {},
   "source": [
    "Let's remind ourselves how big this dataset is (# of points and # of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65462b-3836-451b-af16-6b761234d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {X.shape[0]} data points in the impurity dataset\")\n",
    "print(f\"There are {X.shape[1]} features in the impurity dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4150f3-1d4d-4574-9090-d7dcf3c48eb3",
   "metadata": {},
   "source": [
    "Let's also make a simple 80/20 train/test split. This is important since some of the data analysis techniques will be used to build better supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce4f88-2515-49a5-80d9-3ddb94d4823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bce40-89eb-44e6-9620-d4e5e8bfa33a",
   "metadata": {},
   "source": [
    "One of the challenges with data with 40 dimensions is that it's extremely hard to visualize. 2-3 dimensions is pretty straightforward, but 40 is impossible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04dfe11-1991-4cef-a113-935592cd3adc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1ee44-c2aa-4b16-b1f1-5d069f44ad62",
   "metadata": {},
   "source": [
    "Unlike working with a single variable where we can plot \"x vs. y\", but it is difficult to get a feel for higher-dimension data since it is hard to visualize. One good thing to start with is looking at histograms of each input variable. This is super easy using dataframes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1d655-59ff-46b5-b875-77bad9149c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[1:41]].hist(bins=30, figsize=(30, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d09224-14b9-468c-88ba-7aae4a4e395d",
   "metadata": {},
   "source": [
    "We can see that some features are normally distributed, while others have some obvious outliers or bimodal distribution. We have no idea how these features are correlated yet - that is if any of them are related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908109e-070c-4510-b4ae-10051ba87c1b",
   "metadata": {},
   "source": [
    "## Feature correlations\n",
    "\n",
    "There are 40 features in the dataset, but from our engineering knowledge we expect that some might end up being correlated. For example, if there's an energy balance, the energy in one unit may be directly correlated with the energy in another unit or stream. \n",
    "\n",
    "We can formalize this with a **correlation coefficient**. The most common/useful correlation coefficient is the Pearson correlation coefficient. It is a number in the range [-1,1] that describes how correlated two variables are. I find this plot from the wikipedia page to be extremely helpful!\n",
    "\n",
    "![correlation examples from wikipedia article](resources/Correlation_examples2.svg)\n",
    "\n",
    "`````{seealso}\n",
    "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "`````\n",
    "\n",
    "Let's try this for our data! `df.corr()` calculates the correlation coefficient for all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171e08b-7132-4be7-922e-c0bca666ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Found this snippet by googling \"correlation matrix plotly\"\n",
    "# and finding https://stackoverflow.com/questions/66572672/correlation-heatmap-in-plotly\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_heatmap(\n",
    "    z=df[df.columns[1:41]].corr(),\n",
    "    x=x_names,\n",
    "    y=x_names,\n",
    "    colorscale=px.colors.diverging.RdBu,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    ")\n",
    "fig.update_layout(autosize=False, width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc961717-ccc3-4d7d-b95c-5c0a5d82e703",
   "metadata": {},
   "source": [
    "This plot has some very interesting structure!\n",
    "* The diagonal is 1 - every feature is perfectly correlated with itself\n",
    "* The plot is symmetric - the correlation between x1 and x2 is the same as the correlation between x2 and x1\n",
    "* Many pairs have a strong positive (~1) correlation\n",
    "* Some pairs have a very weak correlation\n",
    "* A few pairs have strong negative correlation\n",
    "* There are some obvious groups among the features. For example, all of the primary column bed temperatures are strongly correlated with each other.\n",
    "\n",
    "\n",
    "Let's plot a few of these just to see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadb93e-7a2e-45a5-9edc-7e6dbb92904c",
   "metadata": {},
   "source": [
    "### Example of positive correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc934e-3dff-470d-ab42-de9842f60bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"x18:Primary Column Bed 4 Temperature\"],\n",
    "    y=df[\"x19:Primary Column Bed 3 Temperature\"],\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_layout(autosize=False, width=600, height=600)\n",
    "fig.update_xaxes(title_text=\"x18:Primary Column Bed 4 Temperature\")\n",
    "fig.update_yaxes(title_text=\"x19:Primary Column Bed 3 Temperature\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab3dc3-ebae-4305-a886-1e5daa153dc6",
   "metadata": {},
   "source": [
    "It's probably not so surprising that the temperature in beds next to each other in the same column are pretty strongly correlated! \n",
    "\n",
    "Interestingly there are a few strong outliers here - that could either be noise or erroneous datapoints, or could be really interesting and rare scenarios. We don't really know unless we dig into the actual data. I would probably select a few of those conditions and investigate what happened at those specific times!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af291e36-0e45-4bab-bd5f-846a11bff573",
   "metadata": {},
   "source": [
    "### Example of negative correlation\n",
    "Let's try one of the negative ones from the matrix above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0031f9-6fc0-4cc3-9371-d9f5e4224789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"x34: Secondary Column Tails Temperature\"],\n",
    "    y=df[\"x22: Secondary Column Base Concentration\"],\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_layout(autosize=False, width=600, height=600)\n",
    "fig.update_xaxes(title_text=\"x34: Secondary Column Tails Temperature\")\n",
    "fig.update_yaxes(title_text=\"x22: Secondary Column Base Concentration\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b78386-70bf-41df-8a83-5bdb1f97a49c",
   "metadata": {},
   "source": [
    "This one is a little less clear - there's clearly a correlated. All of the points with high tails temperature in the second column also have a very low base concentration. I can't explain this without thinking a little more about the chemical engineering process, but it immediately jumps out from the data. \n",
    "\n",
    "Be careful though - correlation is not causation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c9648-9134-4471-9997-5e05302947d2",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "We can take advantage of the fact that many of the features are correlated to reduce the number of features in our system. From the example above, we probably don't need all of the temperatures in the beds of the first column, unless those outliers happen to be important!\n",
    "\n",
    "Many dimensionality reduction techniques are implemented in scikit-learn. We'll try just two simple ones here.\n",
    "\n",
    "### Practical uses of dimensionality reduction\n",
    "\n",
    "There are a number of practical uses for dimensionality reduction algorithms:\n",
    "\n",
    "* compression of data\n",
    "* denoising of data\n",
    "* interpretation of data\n",
    "* improving model efficiency or performance\n",
    "\n",
    "We will focus primarily on the ways that dimensionality reduction can aid in interpretation and improving model efficiency and performance, but the algorithms used for other applications are the same or similar.\n",
    "\n",
    "### Considerations for dimensionality reduction\n",
    "\n",
    "There are many different kinds of dimensionality reduction approaches, and when selecting between them there are a few things to consider. The relative importance of these factors will depend on the nature of the dataset and the goal of the analysis.\n",
    "\n",
    "* Matrix rank - how many independent dimensions are there?\n",
    "* Linearity of the low-dimensional subspace - are patterns linear or non-linear?\n",
    "* Projection - can a new high-dimensional point be projected onto the low-dimensional map?\n",
    "* Inversion - can a new low-dimensional point be projected back into high-dimensional space?\n",
    "* Supervised vs. unsupervised - are the training labels used to determine the reduced dimensions?\n",
    "\n",
    "### Assessing performance of dimensionality reduction models\n",
    "\n",
    "It can be challenging to assess the performance of dimensional reduction models, especially when unsupervised. Nonetheless there are a few approaches that can be used. Selecting the right approach will depend on the problem, but using a variety of assessment criteria is always a good idea if possible.\n",
    "\n",
    "#### Explained Variance (most common) \n",
    "One common idea in dimensional reduction is to assess the \"explained variance\" of the high-dimensional data. This is common in techniques such as PCA.\n",
    "\n",
    "#### Distance\n",
    "\n",
    "The \"stress\" function compares the distance between points $i$ and $j$ in a low-dimensional space to the distance in the full-dimensional space:\n",
    "\n",
    "$ S(\\vec{x}_{0}, \\vec{x}_1, \\vec{x}_2, ... \\vec{x}_n) =  \\left( \\frac{\\sum_{i=0}^n \\sum_{i<j}(d_{ij} - ||x_i - x_j||)^2}{\\sum_{i=0}^n \\sum_{i<j} d_{ij}^2} \\right)^{1/2} $\n",
    "\n",
    "\n",
    "where $d_{ij}$ is the distance in the high-dimensional space and $\\vec{x}$ is the vector in the low-dimensional space.\n",
    "\n",
    "A conceptually similar way to express this is:\n",
    "\n",
    "$\\sum_i \\sum_j || d(\\vec{x}_i, \\vec{x}_j) - d(P(\\vec{x}_i), P(\\vec{x}_j))||$\n",
    "\n",
    "where $d(\\vec{x}_i, \\vec{x}_j)$ is the distance between $\\vec{x}_i$ and $\\vec{x}_j$ in the high-dimensional space, and $P(\\vec{x}_j)$ is the reduced-dimension vector.\n",
    "\n",
    "Some approaches seek to minimize these distances directly (e.g. multi-dimensional scaling), but it can also be used as an accuracy metric. We can implement this using a few helper functions. You don't need to worry about the details of this function, but can look up the documentation to see the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcac033-ecaa-4cec-9091-a291a872e5f9",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "You can identify linear combinations of the original features that contain independent information using principal component analysis (PCA). PCA works by using the eigenvectors of the covariance matrix to identify linear combinations. \n",
    " The eigenvectors of the covariance matrix identify the \"natural\" coordinate system of the data.\n",
    "\n",
    "![correlation examples from wikipedia article](resources/PCA.gif)\n",
    "\n",
    "\n",
    "\n",
    "PCA isn't too hard to implement from scratch, but we'll use the sklearn interface since the details are not so important. Unsupervised methods like PCA have a similar interface to sklearn ML models, but instead of a `predict` function they have a `transform` function.\n",
    "\n",
    "`````{seealso}\n",
    "sklearn PCA: https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "\n",
    "scatter matrix: https://plotly.com/python/splom/\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9d60e-2545-4d79-b324-a17eea17e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Default PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA and transform the data\n",
    "components = pca.fit_transform(X_train)\n",
    "\n",
    "# Get the explained variance from the PCA object\n",
    "# and format it as a string\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    hover_data={\n",
    "        a: b for a, b in zip(x_names, X_train.T)\n",
    "    },  # add the rest of the data on hover!\n",
    "    dimensions=range(4),\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.update_layout(autosize=False, width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ceb1c3-b92c-4c97-8052-0b98ab1032ce",
   "metadata": {},
   "source": [
    "Notice that the first three principal components explain almost all of the data as most of the features are correlated. We could use these three components as features in a supervised ML model. You will try that in your homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112998-8604-4ed2-ad73-cd32ca147176",
   "metadata": {},
   "source": [
    "### t-SNE manifold learning\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is another popular method for learning low-dimensional representation of high-dimensional data. t-SNE operates by trying to find a low-dimensional representation that minimizes the stress above. Effectively, you want to learn a manifold such that points that are close in the new space are also close in the high-dimensional space. It is particularly helpful for clustering high-dimensional data.\n",
    "\n",
    "As the name implies, tSNE is stochastic, which means that the results you get will change each time you run it unless you  set the seed.\n",
    "\n",
    "Let's see how it does! This code takes a while to run (~2min).\n",
    "\n",
    "`````{seealso}\n",
    "tSNE in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "Original tSNE paper (2008): https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458a432-1818-4c16-9c4d-69c03e2a6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Default PCA object\n",
    "tsne = TSNE(init=\"pca\", n_iter=1000, verbose=2)\n",
    "\n",
    "# Fit the PCA and transform the data\n",
    "components = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9e497-1b60-431c-97c8-f831aaac7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with plotly!\n",
    "fig = px.scatter(\n",
    "    x=components[:, 0],\n",
    "    y=components[:, 1],\n",
    "    hover_data={\n",
    "        a: b for a, b in zip(x_names, X_train.T)\n",
    "    },  # add the rest of the data on hover!\n",
    "    color=y_train,\n",
    ")\n",
    "fig.update_layout(autosize=False, width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfae1e4-b3c4-4636-926f-0a86d77418fe",
   "metadata": {},
   "source": [
    "We can see that there are some significant clusters that have emerged, and if you zoom in the larger clusters also have clusters. Probably this is coming from multiple data points collected near each other in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5548f-122e-45c2-a7d7-f45af60523bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
