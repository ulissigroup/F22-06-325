{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fa3daf-7464-42f0-a337-bfda7211cb5c",
   "metadata": {},
   "source": [
    "```{margin} Adaptation!\n",
    "This lecture was adapted from the scikit-learn MOOC, which is available under the CC-by 4.0 license here: https://github.com/INRIA/scikit-learn-mooc\n",
    "\n",
    "The penguin dataset is available under the CC-0 license and available from https://github.com/allisonhorst/palmerpenguins\n",
    "\n",
    "Artwork by @allison_horst!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93624a2c-66e9-4445-905c-e431f1981a64",
   "metadata": {},
   "source": [
    "`````{note}\n",
    "This lecture is going to:\n",
    "* Introduce a new dataset with categorical variables (penguins!)\n",
    "* Demonstrate the use of a linear model (logistic regression) for binary classification problems\n",
    "* Show how we can visualize the binary classification problems\n",
    "* Discuss the primary metric for binary classification problems (accuracy)\n",
    "`````\n",
    "\n",
    "`````{seealso}\n",
    "* https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "* https://www.youtube.com/watch?v=het9HFqo1TQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=3\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff6486-8549-4378-925f-324c8f6370ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear models for classification\n",
    "In regression, we saw that the target to be predicted was a continuous\n",
    "variable. In classification, this target will be discrete (e.g. categorical).\n",
    "\n",
    "Before we jump in to our first linear model, let's talk about a new dataset (that could be somewhat relevant to BME double majors who have taken physiology!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c34571-56c4-46cd-ae80-09be82f48f6e",
   "metadata": {},
   "source": [
    "##  The penguins datasets\n",
    "\n",
    "To demonstrate linear classification models we'll use the \n",
    "[Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/)\n",
    "dataset. We use this dataset for both classification and regression\n",
    "problems by selecting a subset of the features to make our explanations\n",
    "intuitive.\n",
    "\n",
    "We will use this dataset in classification setting to predict the penguins'\n",
    "species from anatomical information.\n",
    "\n",
    "Each penguin is from one of the three following species: Adelie, Gentoo, and\n",
    "Chinstrap. See the illustration below depicting the three different penguin\n",
    "species:\n",
    "\n",
    "![Image of penguins](https://github.com/allisonhorst/palmerpenguins/blob/main/man/figures/lter_penguins.png?raw=true)\n",
    "\n",
    "This problem is a classification problem since the target is categorical.\n",
    "We will limit our input data to a subset of the original features\n",
    "to simplify our explanations when presenting the decision tree algorithm.\n",
    "Indeed, we will use features based on penguins' culmen (bill) measurement. You can\n",
    "learn more about the penguins' culmen (bill) with the illustration below:\n",
    "\n",
    "![Image of culmen](https://github.com/allisonhorst/palmerpenguins/raw/main/man/figures/culmen_depth.png)\n",
    "\n",
    "\n",
    "We will start by loading this subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e912a8-5b8b-49ec-b50c-463cdd801823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"datasets/penguins_classification.csv\")\n",
    "\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ca8c5-2723-4639-a9ba-01cfbdd6a822",
   "metadata": {},
   "source": [
    "Let's check the dataset more into details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdf78e-a9de-45fb-9da0-3f7fdc7fa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3abb5-aad9-4cc4-8d7f-2e19ea869e4a",
   "metadata": {},
   "source": [
    "Since that we have few samples, we can check a scatter plot to observe the\n",
    "samples distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f8c0d-0654-48e9-8d58-8a9264dcccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "pairplot_figure = sns.pairplot(penguins, hue=\"Species\")\n",
    "pairplot_figure.fig.set_size_inches(9, 6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d659-ffd5-4ca4-a677-98da86e796b8",
   "metadata": {},
   "source": [
    "First let's check the feature distributions by looking at the diagonal plots\n",
    "of the pairplot. We can deduce the following intuitions:\n",
    "\n",
    "* The Adelie species can be differentiated from the Gentoo and Chinstrap\n",
    "  species depending on the culmen length;\n",
    "* The Gentoo species can be differentiated from the Adelie and Chinstrap\n",
    "  species depending on the culmen depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd174fc9-3341-4109-a8c5-402fbdabfb67",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We will try to predict the penguin species using the culmen information. We will also\n",
    "simplify our classification problem by selecting only 2 of the penguin\n",
    "species (Adelie and Chinstrap) to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf8c0f-a0a1-4884-9454-0b529ff26511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"datasets/penguins_classification.csv\")\n",
    "\n",
    "# only keep the Adelie and Chinstrap classes\n",
    "penguins = penguins.set_index(\"Species\").loc[[\"Adelie\", \"Chinstrap\"]].reset_index()\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bd291-4092-40f4-9acb-d391cca79a98",
   "metadata": {},
   "source": [
    "We can quickly start by visualizing the feature distribution by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53631500-fca0-4280-941b-8a2403dc458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for feature_name in culmen_columns:\n",
    "    plt.figure()\n",
    "    # plot the histogram for each specie\n",
    "    penguins.groupby(\"Species\")[feature_name].plot.hist(alpha=0.5, legend=True)\n",
    "    plt.xlabel(feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e649c-a612-4d1b-92b3-0f6a777c83ec",
   "metadata": {},
   "source": [
    "We can observe that we have quite a simple problem. When the culmen\n",
    "length increases, the probability that the penguin is a Chinstrap is closer\n",
    "to 1. However, the culmen depth is less helpful for predicting the penguin\n",
    "species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cf06e-6f19-45b3-b547-c6edbac24d6f",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model fitting, we will separate the target from the data and\n",
    "we will create a training and a testing set. We could make a validation set as well if we wanted to be a bit more careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5c7d1-16e7-4f85-9d6e-3d8b6b9ca74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "penguins_train, penguins_test = train_test_split(penguins, random_state=0)\n",
    "\n",
    "data_train = penguins_train[culmen_columns]\n",
    "data_test = penguins_test[culmen_columns]\n",
    "\n",
    "target_train = penguins_train[target_column]\n",
    "target_test = penguins_test[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0129c-4dcc-4d1c-818c-d32f70a1b964",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive model (guess the most common label)\n",
    "\n",
    "To get some intuition, let's try the simplest possible model we can think of using just the labels. We will make find the most common type of penguin in the dataset, and always guess that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6648d25-129d-420b-b1d7-8595a279c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "most_common_penguin = target_train.mode()[0]\n",
    "\n",
    "\n",
    "def naivemodel(X):\n",
    "    return [most_common_penguin] * X.shape[0]\n",
    "\n",
    "\n",
    "print(naivemodel(np.array([[1, 1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba4f18-752d-44d4-b7d0-0f3709cdc2d3",
   "metadata": {},
   "source": [
    "## Metrics for binary classification problems\n",
    "\n",
    "**accuracy** is the primary metric for classification problems, and is the percentage of the data that is correctly classified by the model.\n",
    "\n",
    "`````{seealso}\n",
    "Statistics for binary classification problems is a complicated area, especially if the dataset is imbalanced (see the danger note at the end of the lecture) or if there is prior information known about the classification. There are more metrics that can be used in these cases, most of which are various combinations of the true positive, false positive, true negative, false negative statistics:\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "`````\n",
    "\n",
    "We can do this by hand or using sklearn's built-in scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dff3b-ecd3-42da-af6c-4ddb057a3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_classification = target_test == naivemodel(data_test)\n",
    "accuracy = correct_classification.sum() / len(correct_classification)\n",
    "print(f\"Accuracy with custom code: {accuracy*100:.1f}%\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sklearn_accuracy = accuracy_score(target_test, naivemodel(data_test))\n",
    "\n",
    "print(f\"Accuracy with sklearn scoring code: {sklearn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef7db8-3c31-4c7d-9515-298b943462d4",
   "metadata": {},
   "source": [
    "~62% of the penguins in the dataset set were Adelie, so our naive model (no ML required!) gets 62% correct. We should be able to do better than this if our ML model has any value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95a920-41c1-4dbb-a23f-f536bf796084",
   "metadata": {},
   "source": [
    "## Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e17775-dd0f-4efc-aecc-099617357141",
   "metadata": {},
   "source": [
    "\n",
    "The linear regression that we have seen so far  will predict a continuous\n",
    "output. When the target is a binary outcome, one can use the logistic\n",
    "function to model the probability. This model is known as logistic\n",
    "regression.\n",
    "\n",
    "Scikit-learn provides the class `LogisticRegression` which implements this\n",
    "algorithm.\n",
    "\n",
    "`````{seealso}\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* https://en.wikipedia.org/wiki/Logistic_regression\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3df950-7193-40c6-ab53-33a2f24828dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def89af-b606-423c-a34b-68e06aec93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(penalty=\"none\")\n",
    ")\n",
    "logistic_regression.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467973c-5dfc-463e-a1c3-674d7f312dc5",
   "metadata": {},
   "source": [
    "We can calculate our model performance using the scoring function attached to the model. This is basically the same as predicting on the test set and using the scoring function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe6cb2-7cde-4bec-8417-5d25a2bbb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = logistic_regression.score(data_test, target_test)\n",
    "print(f\"Accuracy on test set: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b303e41-f7c1-4fb1-90a7-352c9d83c994",
   "metadata": {},
   "source": [
    "\n",
    "Since we are dealing with a classification problem containing only 2\n",
    "features, it is then possible to observe the decision function boundary.\n",
    "The boundary is the rule used by our predictive model to affect a class label\n",
    "given the feature values of the sample.\n",
    "\n",
    "`````{note}\n",
    "Here, we will use the class DecisionBoundaryDisplay. This educational tool\n",
    "allows us to gain some insights by plotting the decision function boundary\n",
    "learned by the classifier in a 2 dimensional feature space.</p>\n",
    "<p class=\"last\">Notice however that in more realistic machine learning contexts, one would typically fit on more than two features at once and therefore it would not be possible to display such a visualization of the decision boundary in general.\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a47c08-d0b8-4b34-acb7-26e7976c0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    logistic_regression, data_test, response_method=\"predict\", cmap=\"RdBu_r\", alpha=0.5\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=penguins_test,\n",
    "    x=culmen_columns[0],\n",
    "    y=culmen_columns[1],\n",
    "    hue=target_column,\n",
    "    palette=[\"tab:red\", \"tab:blue\"],\n",
    ")\n",
    "_ = plt.title(\"Decision boundary of the trained\\n LogisticRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995caf3-f565-4fa6-be18-e14aec54e66d",
   "metadata": {},
   "source": [
    "Thus, we see that our decision function is represented by a line separating\n",
    "the 2 classes. We should also note that we did not impose any regularization\n",
    "by setting the parameter `penalty` to `'none'`.\n",
    "\n",
    "Since the line is oblique, it means that we used a combination of both\n",
    "features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04138d-42ba-4f5a-aec0-07ec756ffe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = logistic_regression[-1].coef_[0]  # the coefficients is a 2d array\n",
    "weights = pd.Series(coefs, index=culmen_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15898e-cbae-4077-ada5-b811c672f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.plot.barh()\n",
    "_ = plt.title(\"Weights of the logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff271c-3888-4a4d-b356-cf6743a1312a",
   "metadata": {},
   "source": [
    "Indeed, both coefficients are non-null. If one of them had been zero, the\n",
    "decision boundary would have been either horizontal or vertical.\n",
    "\n",
    "Furthermore the intercept is also non-zero, which means that the decision does\n",
    "not go through the point with (0, 0) coordinates.\n",
    "\n",
    "For the mathematically inclined reader, the equation of the decision boundary\n",
    "is:\n",
    "\n",
    "    coef0 * x0 + coef1 * x1 + intercept = 0\n",
    "\n",
    "where `x0` is `\"Culmen Length (mm)\"` and `x1` is `\"Culmen Depth (mm)\"`.\n",
    "\n",
    "This equation is equivalent to (assuming that `coef1` is non-zero):\n",
    "\n",
    "    x1 = coef0 / coef1 * x0 - intercept / coef1\n",
    "\n",
    "which is the equation of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f197176-0a66-4b0d-975f-1910de38ff5a",
   "metadata": {},
   "source": [
    "`````{danger}\n",
    "\n",
    "Skeptical chemical engineering students might notice that the metric we're using here (**accuracy**) here can lead to problems if the dataset is imbalanced.\n",
    "\n",
    "Imagine a dataset of where there were 1000 penguins, 999 of which were Adelie and 1 of which was Chinstrap. In this case, a naive model that always predicted \"Adelie\", regardless of the penguin features, would be 99.9% correct! At first glance the model would sound very impressive, but it would actually be worthless for predicting Chinstrap penguins.\n",
    "\n",
    "If you're dealing with strongly imbalanced, you should think carefully about the statistics:\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "* https://arxiv.org/ftp/arxiv/papers/2108/2108.00071.pdf\n",
    "`````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
