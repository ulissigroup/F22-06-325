{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770426ad-dc30-4b68-805f-94f7f13700fb",
   "metadata": {},
   "source": [
    "```{margin} Adaptation!\n",
    "Some of this lecture was inspired by or uses content from Prof. AJ Medford (GTech)'s lectures for ChBE4745: https://github.com/medford-group/data_analytics_ChE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ce71f-85a9-4f4f-9084-2664a2e49e9a",
   "metadata": {},
   "source": [
    "`````{note}\n",
    "This lecture is going to:\n",
    "* Discuss the concept of parametric vs non-parametric models\n",
    "* Introduce a new dataset for spectra fitting problems that you might see if using analytical chemistry techniques\n",
    "* Summarize several of the most common non-parametric models \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44122e1-429b-4bba-8a73-204f580bb048",
   "metadata": {},
   "source": [
    "# Non-parametric models\n",
    "\n",
    "**Parametric models** fit parameters within models to represent some underlying data. Everything we've done so far has been a parametric model. A few examples:\n",
    "* Linear regression\n",
    "* Polynomial regression (which is really linear regression with polynomial features)\n",
    "* Logistic regression\n",
    "* Non-linear regression (like we implemented with `lmfit`)\n",
    "* Neural networks\n",
    "* ...\n",
    "\n",
    "**Non-parametric models** don't explicitly fit parameters, but use other approaches to predict data. A few examples, some of which we will cover today:\n",
    "* Decision trees\n",
    "* kernel regression\n",
    "* K-nearest neighbors (KNN)\n",
    "* Gaussian Processes\n",
    "* ...\n",
    "\n",
    "Parametric models tend to work best when you have a lot of data, and conversely non-parametric models tend to work better with small amounts of data. Some of this is because of differences in flexibility of the models, and part is because non-parametric models tend to get more computationally expensive with increasingly large datasets! \n",
    "\n",
    "Because non-parametric models are not learning underlying patterns in the dataset, they also tend to have trouble extrapolating to data that is different from the training dataset. \n",
    "\n",
    "`````{seealso}\n",
    "https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c031bd-807f-459a-95fc-cd02bb5a01b3",
   "metadata": {},
   "source": [
    "## Example dataset - IR spectra\n",
    "\n",
    "Let's consider fitting models to experimental IR spectra. These models might be useful if you have an experimental measurement that is a bit noisy, or perhaps you want to predict the spectra in between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afe7ce-1d02-4af8-9b65-03933f82505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv(\"data/ethanol_IR.csv\")\n",
    "\n",
    "fig = px.line(df, x=\"wavenumber [cm^-1]\", y=\"absorbance\", title=\"Ethanol IR Spectra\")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd73513-bd47-4c64-bde0-ac7c5f5d27c2",
   "metadata": {},
   "source": [
    "Before we jump in, a few interesting things about the dataset\n",
    "* Some of the absorbance values are negative (!!)\n",
    "* There's a number of different peaks\n",
    "* It's unclear if the noise is uniform everywhere (e.g. the IR spectra at lower wavenumbers looks more noisy than at higher wavenumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c4249-944b-4107-a2bf-0eff0e60a080",
   "metadata": {},
   "source": [
    "## Train/Val/Test split (given every 10th spectra, predict the rest)\n",
    "\n",
    "As always, let's start with a train/val/test split. To make this interesting, let's to 10/45/45 train/val/test. We'll sample the training set uniformly, so that this is representative of a challenge where we have a low-quality IR spectra and want to predict a high-quality one.\n",
    "\n",
    "That is, given one one tenth of the data, predict the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65ba7b-6999-4ba4-bbe2-204de5a8e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = df[df.index % 10 == 0]\n",
    "df_valtest = df[df.index % 10 != 0]\n",
    "df_val, df_test = train_test_split(df_valtest, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7088a9-1044-4838-8466-3b9e5776212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_test[\"wavenumber [cm^-1]\"],\n",
    "    y=df_test[\"absorbance\"],\n",
    "    name=\"Test data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f6274-528f-4c0a-a8b8-75dd2f374d25",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "One of the strategies we discussed for predicting values of unseen data is to simply look at the nearest training data and average those predictions. This strategy is known as K-Nearest Neighbors (KNN). **K** is the number of neighbors that we want to average over.\n",
    "\n",
    "This is very easy to implement in sklearn! Non-parametric models implement the same interface, and are required to have `fit` and `predict` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462f8f1-e4aa-4a16-9b4b-cc20a9936eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize and fit a KNN model\n",
    "# n_neighbors here is the number of numbers to use in the prediction\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# KNN has a fit function like every other supervised method in sklearn\n",
    "model.fit(\n",
    "    X=df_train[\"wavenumber [cm^-1]\"].values.reshape((-1, 1)),\n",
    "    y=df_train[\"absorbance\"].values.reshape((-1, 1)),\n",
    ")\n",
    "\n",
    "val_MAE = mean_absolute_error(\n",
    "    df_val[\"absorbance\"].values.reshape((-1, 1)),\n",
    "    model.predict(df_val[\"wavenumber [cm^-1]\"].values.reshape((-1, 1))),\n",
    ")\n",
    "print(f\"The MAE for this KNN is {val_MAE:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb7cd8-cf0d-4228-be82-b2900e6c97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "X_eval = np.arange(250, 4000, 0.1)\n",
    "fig.add_scatter(\n",
    "    x=X_eval,\n",
    "    y=model.predict(X_eval.reshape((-1, 1))).reshape(\n",
    "        (-1)\n",
    "    ),  # Some tricks to make sure this is a vector\n",
    "    name=\"K Neighbor Regression spectra\",\n",
    "    line_color=\"#ff0000\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e35786-65ce-496b-a8a3-fc5bc697d587",
   "metadata": {},
   "source": [
    "`````{warning}\n",
    "Note that KNN with uniform weights yields a piecewise function with zero derivatives everywhere (except at the discontinuities). This could lead to very unhelpful optimization results if you're trying to minimize or maximize the function/model!\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c370a-82b3-43d5-96ec-d5fe45430ffb",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Try varying the number of neighbors and the `weights` parameter. How does this change the results? Can you get better validation results then above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce55cba-4706-42c6-9a6f-05b377b682d9",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are a non-parametric model that recursively splits the data. The general strategy is something like:\n",
    "* Start with all data in the same bin\n",
    "* Choose a random (or best) feature. Find the value of the feature that best separates the bin into two bins with large difference in mean values\n",
    "* Repeat for all bins until the bins are a minimum size or progress is not being made, or we hit a max depth (many possible choices here!)\n",
    "One really nice advantage of this process is that we can examine the learned rules to see how the model is making its decisions!\n",
    "\n",
    "Let's see this in practice using sklearn!\n",
    "\n",
    "`````{seealso}\n",
    "* https://scikit-learn.org/stable/modules/tree.html#decision-trees\n",
    "* https://inria.github.io/scikit-learn-mooc/trees/trees_module_intro.html\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8918442-328c-447b-a61b-3c0f676e796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "model.fit(\n",
    "    X=df_train[\"wavenumber [cm^-1]\"].values.reshape((-1, 1)),\n",
    "    y=df_train[\"absorbance\"].values.reshape((-1, 1)),\n",
    ")\n",
    "\n",
    "val_MAE = mean_absolute_error(\n",
    "    df_val[\"absorbance\"].values.reshape((-1, 1)),\n",
    "    model.predict(df_val[\"wavenumber [cm^-1]\"].values.reshape((-1, 1))),\n",
    ")\n",
    "print(f\"The MAE for the default decision tree regressor is {val_MAE:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb70d95-a68c-4030-b8a2-3d14b9b37234",
   "metadata": {},
   "source": [
    "The accuracy is quite a bit better than the KNN above! Obviously this is not always the case, but it's pretty common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63ee74-9bfa-4b91-a632-60e306e8a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "X_eval = np.arange(250, 4000, 0.1)\n",
    "fig.add_scatter(\n",
    "    x=X_eval,\n",
    "    y=model.predict(X_eval.reshape((-1, 1))).reshape(\n",
    "        (-1)\n",
    "    ),  # Some tricks to make sure this is a vector\n",
    "    name=\"Decision Tree Regressor spectra\",\n",
    "    line_color=\"#ff0000\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef84b3-a475-4543-9992-8322fb686a27",
   "metadata": {},
   "source": [
    "Notice that decision trees also have the property that all points within a bin end up with the same value, which means that the gradient everywhere is zero. We expect the spectra to be somewhat smooth, so this makes the predictions look a bit strange!\n",
    "\n",
    "### Visualizing decision trees\n",
    "\n",
    "We can visualize the decisions made in the model. To keep this easy to read we'll limit the tree to a max depth of 2. If we had many possible features, these splits would also help us understand which features were most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2fd34-6178-43f4-b023-a256d504c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=2)\n",
    "\n",
    "# KNN has a fit function like every other supervised method in sklearn\n",
    "model.fit(\n",
    "    X=df_train[\"wavenumber [cm^-1]\"].values.reshape((-1, 1)),\n",
    "    y=df_train[\"absorbance\"].values.reshape((-1, 1)),\n",
    ")\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "tree.plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efd114-c996-4ced-b99d-3930b3942c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "X_eval = np.arange(250, 4000, 0.1)\n",
    "fig.add_scatter(\n",
    "    x=X_eval,\n",
    "    y=model.predict(X_eval.reshape((-1, 1))).reshape(\n",
    "        (-1)\n",
    "    ),  # Some tricks to make sure this is a vector\n",
    "    name=\"Decision Tree Regressor spectra\",\n",
    "    line_color=\"#ff0000\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6cc97-90e3-459b-a337-2a61acdd4e0e",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Try varying the max_depth and the `criterion` parameter. How does this change the results? Can you get better validation results then above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc94c2-4d9d-4417-bd1d-3a8bad035c29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random forests (a bunch of random trees)\n",
    "\n",
    "The decisions made in the decision tree process are usually stochastic. If you look at the `DecisionTreeRegressor`, you'll see there's also an argument `random_state` to control it's outputs. \n",
    "\n",
    "We can take advantage of this randomness to fit an ensemble of many similar decision trees, then report the average and standard deviation of that  ensemble to estimate uncertainty in the model predictions. This type of model is called a **random forest**. The same idea holds with many other stochastic models, but is particularly popular for decision trees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626df3fa-1e29-4c46-9ab5-052b93a0045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# model has a fit function like every other supervised method in sklearn\n",
    "model.fit(\n",
    "    X=df_train[\"wavenumber [cm^-1]\"].values.reshape((-1, 1)),\n",
    "    y=df_train[\"absorbance\"].values.reshape((-1, 1)),\n",
    ")\n",
    "\n",
    "val_MAE = mean_absolute_error(\n",
    "    df_val[\"absorbance\"].values.reshape((-1, 1)),\n",
    "    model.predict(df_val[\"wavenumber [cm^-1]\"].values.reshape((-1, 1))),\n",
    ")\n",
    "print(f\"The MAE for the default decision tree regressor is {val_MAE:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab6fb1-c7f5-4f14-856e-991e93d5fd61",
   "metadata": {},
   "source": [
    "Note the validation MAE here is a bit better than a single decision tree. This is a common pattern in machine learning and a technique that can be used to improve predictive power - fit an ensemble many identical models with different random initializations and average the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7240e-099e-4f46-b824-b05297dba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "X_eval = np.arange(250, 4000, 0.1)\n",
    "fig.add_scatter(\n",
    "    x=X_eval,\n",
    "    y=model.predict(X_eval.reshape((-1, 1))).reshape(\n",
    "        (-1)\n",
    "    ),  # Some tricks to make sure this is a vector\n",
    "    name=\"Decision Tree Regressor spectra\",\n",
    "    line_color=\"#ff0000\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625b336-d4e8-461d-8284-3042e1d099f7",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Try varying the `n_estimators` and `max_depth` parameters. How does this change the results? Can you get better validation results then above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690a882-ec7e-4cfc-993a-3a8e301b3a51",
   "metadata": {},
   "source": [
    "## Gaussian process regression\n",
    "\n",
    "Gaussian processes are one of my favorite model types because they have many nice properties\n",
    "* They are exact (no fitting required)\n",
    "* They provide uncertainty estimates\n",
    "* They let you control how data is related\n",
    "* They make you think about what it means to be similar in the feature space\n",
    "* They are (usually) fast\n",
    "* They extrapolate in a controllable way\n",
    "* They are Bayesian and allow you to encode a prior belief\n",
    "\n",
    "We could spend a whole semester talking about these models, but we'll just cover the high-level choices here. Gaussian processes model predictions as an infinite ensemble of possible Bayesian predictions The most important design decision in a GP model is the **covariance function** that says how related two hypothetical data points for. The most popular function is the RBF or squared-exponential kernel\n",
    "\\begin{align*}\n",
    "k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n",
    "\\end{align*}\n",
    "where $x_i,x_j$ are feature vectors representing two possible data points, $d(x_i, x_j)$ is the euclidean distance between the two vectors, $l$ is a length scale that can be tuned, and $k(x_1,x_2)$ is the covariance between the two points. Note that this function is basically saying far away points are uncorrelated, and close points are correlated (where \"close\" is >> or << $l$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da2c00-9b50-4a6e-a1fc-8d7a58ebfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ConstantKernel\n",
    "\n",
    "kernel =  ConstantKernel(1) * RBF(40.0)\n",
    "model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "# model has a fit function like every other supervised method in sklearn\n",
    "model.fit(\n",
    "    X=df_train[\"wavenumber [cm^-1]\"].values.reshape((-1, 1)),\n",
    "    y=df_train[\"absorbance\"].values.reshape((-1, 1)),\n",
    ")\n",
    "\n",
    "val_MAE = mean_absolute_error(\n",
    "    df_val[\"absorbance\"].values.reshape((-1, 1)),\n",
    "    model.predict(df_val[\"wavenumber [cm^-1]\"].values.reshape((-1, 1))),\n",
    ")\n",
    "print(f\"The MAE for the default decision tree regressor is {val_MAE:0.4f}\")\n",
    "\n",
    "\n",
    "# The predict function has an extra keyword to also return the standard deviation\n",
    "y_predict, std_predict = model.predict(X_eval.reshape((-1, 1)),return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cd77f-d448-4aae-9ec3-d11b907d9f21",
   "metadata": {},
   "source": [
    "This works even better than the random forest above! However, I had to play around with the parameters (kernel, etc) in order to get that work really well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d04e8-d715-4db5-8507-0e5d0940c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot with plotly!\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=df[\"wavenumber [cm^-1]\"], y=df[\"absorbance\"], name=\"Actual spectra data\"\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_train[\"wavenumber [cm^-1]\"],\n",
    "    y=df_train[\"absorbance\"],\n",
    "    name=\"Train data\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "\n",
    "X_eval = np.arange(250, 4000, 0.1)\n",
    "\n",
    "# The predict \n",
    "y_predict, std_predict = model.predict(X_eval.reshape((-1, 1)),return_std=True)\n",
    "y_predict = y_predict.reshape(\n",
    "        (-1)\n",
    "    )\n",
    "fig.add_scatter(\n",
    "    x=X_eval,\n",
    "    y=model.predict(X_eval.reshape((-1, 1))).reshape(\n",
    "        (-1)\n",
    "    ),  # Some tricks to make sure this is a vector\n",
    "    name=\"Decision Tree Regressor spectra\",\n",
    "    line_color=\"#ff0000\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df_val[\"wavenumber [cm^-1]\"],\n",
    "    y=df_val[\"absorbance\"],\n",
    "    name=\"Validation spectra\",\n",
    "    mode=\"markers\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "        name='Upper Bound',\n",
    "        x=X_eval,\n",
    "        y=y_predict+ std_predict,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#444\"),\n",
    "        line=dict(width=0),\n",
    "        # showlegend=False\n",
    "    )\n",
    "fig.add_scatter(\n",
    "        name='Lower Bound',\n",
    "        x=X_eval,\n",
    "        y=y_predict- std_predict,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"#a00\"),\n",
    "        line=dict(width=0),\n",
    "            fill='tonexty',\n",
    "        # showlegend=False\n",
    "    )\n",
    "fig.update_xaxes(title_text=\"Wavenumber [cm^-1]\")\n",
    "fig.update_yaxes(title_text=\"Absorbence [AU]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15cbf5-8391-403d-8288-7d6f049ec780",
   "metadata": {},
   "source": [
    "`````{danger}\n",
    "Notice that this model claims to have uncertainty estimates. However, the validation data is often outside of the range. This suggests that the uncertainty estimates might not be trustworthy. \n",
    "`````\n",
    "\n",
    "`````{warning}\n",
    "Notice that the model has no problem predicting negative values. The starting data also had values that were very slightly negative so maybe we shouldn't be too worried, but a negative absorbance stands out as a bit suspect!\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04162dae-5aa1-4865-b642-aa331c79622e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
