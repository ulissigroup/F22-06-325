{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65c6d08-f832-4b8e-a920-94e943105ff0",
   "metadata": {},
   "source": [
    "`````{note}\n",
    "This lecture is going to:\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fc47d-152a-4950-a4a7-c531e74ef451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Features and Feature Selection\n",
    "\n",
    "We're now at the point where we may have many features in a dataset, or where we want to generate many features to build predictive models. We need ways to generalize this process, and control how many features are used. \n",
    "\n",
    "As a reminder, we're currently interested in solving supervised regression problems of the form:\n",
    "\\begin{align*}\n",
    "\\hat{y}=f(\\mathbf{X})\n",
    "\\end{align*}\n",
    "where \n",
    "* $\\hat{y}$ is a vector of outputs (one per data point)\n",
    "* $\\mathbf{X}$ is a 2D array of features (one row per data point, one column per feature)\n",
    "\n",
    "We're only going to use linear regression models in today's lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85397e-c74d-48c9-8e41-2d716dc84bfe",
   "metadata": {},
   "source": [
    "## Test function (Himmelblau's function)\n",
    "\n",
    "Let's start with the special function we used for the optimization lecture\n",
    "\\begin{align}\n",
    "f(x, y) = (x^2+y-11)^2+(x+y^2-7)^2\n",
    "\\end{align}\n",
    "For clarity, I'm going to write this as \n",
    "\\begin{align}\n",
    "y=f(x_1, x_2) &= (x_1^2+x_2-11)^2+(x_1+x_2^2-7)^2\\\\\n",
    "&=x_1^4+x_2^4+2x_1x_2^2+2x_2x_1^2-21x_1^2-13x_2^2-14x_1-22x_2+170\n",
    "\\end{align}\n",
    "where y is the output/label, and $x_1,x_2$ are potential features (among many that we could choose)! This is a nice test function since it's analytical and is polynomial. If we try to find polynomial features of $x_1,x_2$, we should recover this form. \n",
    "\n",
    "```{seealso}\n",
    "https://en.wikipedia.org/wiki/Himmelblau%27s_function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777a098-1164-4fc6-99d8-05512fd45eea",
   "metadata": {},
   "source": [
    "First, let's define the range of $x_1,x_2$ values we're interested in plotting over, and turn them into a 2D grid like we did in the optimization lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae3f70-35f7-46d2-81a4-ae39fb6fc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1range = np.linspace(-5, 5)\n",
    "x2range = np.linspace(-5, 5)\n",
    "\n",
    "# Make 2d arrays for all the unique values of x_1/x_2\n",
    "x1grid, x2grid = np.meshgrid(x1range, x2range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560303a-d27b-47ce-a78a-f4acb895a0cd",
   "metadata": {},
   "source": [
    "Now, let's define a function to return Himmelblau's function, plus a little bit of random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb7381-9463-4cac-b5b9-e276e3e2d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def himmelblau_with_noise(x1, x2, noise=0.1, seed=42):\n",
    "\n",
    "    # Set the numpy random seed so the results are reproducible\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate the himmelblau function\n",
    "    himmelblau = (x1**2 + x2 - 11) ** 2 + (x1 + x2**2 - 7) ** 2\n",
    "\n",
    "    # Multiple by a bit of Gaussian random noise and return\n",
    "    noise = np.random.normal(loc=1, scale=noise, size=x1.shape)\n",
    "    return himmelblau * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7ee90-9f0d-4ac8-9a68-faea1b91a358",
   "metadata": {},
   "source": [
    "Finally, let's plot the function without noise, and add points for 20 random points in that space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68fe54-0c66-4277-b186-741865268712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Surface(\n",
    "            x=x1range, y=x2range, z=himmelblau_with_noise(x1grid, x2grid, noise=0)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate 20 samples from the noisy Himmelblau function\n",
    "nsamples = 20\n",
    "X = np.random.uniform(low=-5, high=5, size=(nsamples, 2))\n",
    "y = himmelblau_with_noise(X[:, 0], X[:, 1])\n",
    "\n",
    "# Plot with plotly\n",
    "fig.add_scatter3d(\n",
    "    x=X[:, 0], y=X[:, 1], z=y, mode=\"markers\", marker=dict(size=6, color=\"#00FF00\")\n",
    ")\n",
    "fig.update_layout(autosize=False, width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab783bb9-7620-4bbb-b903-93a872fdab84",
   "metadata": {},
   "source": [
    "## Base case ($x_1$, $x_2$ as the only the features)\n",
    "\n",
    "Our goal will be to find the polynomial features that make this an easy function to fit. Before we do something interesting, let's start with the simplest thing we can try: a linear function using just $x_1$ and $x_2$ as features.\n",
    "\n",
    "We'll use scikit-learn now that we've seen an example through the homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489ca17-d297-4146-944b-b37b099a1c18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdb76b-d7ee-4839-aa63-c812264e5616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
